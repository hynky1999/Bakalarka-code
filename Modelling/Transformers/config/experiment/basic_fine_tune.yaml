# @package _global_
data:
  batch_size: 64

optimizer:
  lr: 2e-5
  weight_decay: 0

callbacks:
  - _target_: callbacks.GradualUnfreezingCallback
    unfreeze_per_epoch: 2
    # Unfreeze just last 2 layers then stop -> Training with last 2 layers
    min_unfreeze_layer: 6

defaults:
  - override /data: news_tokenized
  - override /scheduler: linear_warmup
  - override /optimizer: adamW
  - override /model: robeczech_class

max_epochs: 6