{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: datasets in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (2.8.0)\n",
      "Collecting transformers==4.24.0\n",
      "  Using cached transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from transformers==4.24.0) (0.11.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from transformers==4.24.0) (4.64.1)\n",
      "Requirement already satisfied: filelock in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from transformers==4.24.0) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from transformers==4.24.0) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from transformers==4.24.0) (22.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from transformers==4.24.0) (0.13.2)\n",
      "Requirement already satisfied: requests in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from transformers==4.24.0) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from transformers==4.24.0) (1.24.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: wheel in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (58.1.0)\n",
      "Requirement already satisfied: multiprocess in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from datasets) (1.5.2)\n",
      "Requirement already satisfied: aiohttp in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: xxhash in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from requests->transformers==4.24.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from requests->transformers==4.24.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from requests->transformers==4.24.0) (1.26.13)\n",
      "Requirement already satisfied: pytz>=2020.1 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: transformers\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed transformers-4.24.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/lnet/aic/personal/kydliceh/non_runable/NLP_venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch datasets transformers==4.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ufal/robeczech-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "def grad_params(params, freeze=True):\n",
    "    for param in params:\n",
    "        param.requires_grad = freeze\n",
    "\n",
    "\n",
    "\n",
    "def get_model(out_dim=5, freeze_layers=2):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"ufal/robeczech-base\", num_labels=out_dim)\n",
    "    grad_params(model.parameters(), freeze=True)\n",
    "    grad_params(model.classifier.parameters(), freeze=False)\n",
    "    for layer in model.roberta.encoder.layer[-freeze_layers:]:\n",
    "        grad_params(layer.parameters(), freeze=False)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset dataset (/home/kydliceh/.cache/huggingface/datasets/dataset/default/0.0.0/3a21ebffd6f7380b097ffe2f8be2beda23497fd4b6ff06de19b9c473635bb199)\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = load_dataset(\"../../../final_dataset/dataset.py\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset dataset (/home/kydliceh/.cache/huggingface/datasets/dataset/default/0.0.0/3a21ebffd6f7380b097ffe2f8be2beda23497fd4b6ff06de19b9c473635bb199)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"../../../final_dataset/dataset.py\", split=\"train\")\n",
    "train_dataset = train_dataset.remove_columns([\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains a hole for index 51959, your vocabulary could be corrupted !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fad04f3c47349f79301303ec2bf8261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1784 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_function\u001b[39m(examples):\n\u001b[1;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(examples[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m], truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m train_dataset_new \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39;49mmap(preprocess_function, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, keep_in_memory\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,load_from_cache_file\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2826\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2823\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   2825\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2826\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   2827\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   2828\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   2829\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   2830\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   2831\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   2832\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2833\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   2834\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   2835\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   2836\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   2837\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   2838\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   2839\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2840\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   2841\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   2842\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   2843\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   2844\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   2845\u001b[0m     )\n\u001b[1;32m   2846\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2848\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    561\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    563\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    565\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:529\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    523\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    525\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    526\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    527\u001b[0m }\n\u001b[1;32m    528\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    530\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    531\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/datasets/fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    478\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    482\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3264\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   3262\u001b[0m                 writer\u001b[39m.\u001b[39mwrite_table(batch)\n\u001b[1;32m   3263\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3264\u001b[0m                 writer\u001b[39m.\u001b[39;49mwrite_batch(batch)\n\u001b[1;32m   3265\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mand\u001b[39;00m writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3266\u001b[0m     writer\u001b[39m.\u001b[39mfinalize()  \u001b[39m# close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\u001b[39;00m\n",
      "File \u001b[0;32m/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/datasets/arrow_writer.py:552\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    550\u001b[0m         typed_sequence \u001b[39m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mcol_type, try_type\u001b[39m=\u001b[39mcol_try_type, col\u001b[39m=\u001b[39mcol)\n\u001b[1;32m    551\u001b[0m         arrays\u001b[39m.\u001b[39mappend(pa\u001b[39m.\u001b[39marray(typed_sequence))\n\u001b[0;32m--> 552\u001b[0m         inferred_features[col] \u001b[39m=\u001b[39m typed_sequence\u001b[39m.\u001b[39;49mget_inferred_type()\n\u001b[1;32m    553\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n\u001b[1;32m    554\u001b[0m pa_table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n",
      "File \u001b[0;32m/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/datasets/arrow_writer.py:129\u001b[0m, in \u001b[0;36mTypedSequence.get_inferred_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the inferred feature type.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39mThis is done by converting the sequence to an Arrow array, and getting the corresponding\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39mfeature type.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39m    FeatureType: inferred feature type of the sequence.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_type \u001b[39m=\u001b[39m generate_from_arrow_type(pa\u001b[39m.\u001b[39;49marray(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39mtype)\n\u001b[1;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_type\n",
      "File \u001b[0;32m/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/pyarrow/array.pxi:236\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/pyarrow/array.pxi:110\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/datasets/arrow_writer.py:189\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     trying_cast_to_python_objects \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     out \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49marray(cast_to_python_objects(data, only_1d_for_numpy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    190\u001b[0m \u001b[39m# use smaller integer precisions if possible\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrying_int_optimization:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ufal/robeczech-base\")\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"content\"], truncation=True, max_length=512)\n",
    "\n",
    "train_dataset_new = train_dataset.map(preprocess_function, batched=True, keep_in_memory=False,load_from_cache_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains a hole for index 51959, your vocabulary could be corrupted !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e89aecbe8994237a814677d2cfb9671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dataset = eval_dataset.remove_columns([\"category\"])\n",
    "eval_dataset_new = eval_dataset.map(preprocess_function, batched=True, keep_in_memory=False,load_from_cache_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(max_length=512, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(x, column):\n",
    "    x = x.filter(lambda ex: ex[column] is not None)\n",
    "    x = x.rename_column(column, \"labels\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m server_dataset \u001b[39m=\u001b[39m prepare_dataset(train_dataset_new, \u001b[39m\"\u001b[39m\u001b[39mserver\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prepare_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "server_dataset = prepare_dataset(train_dataset_new, \"server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cd8d07b0734d21896e2e7dc8e3922d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "server_eval_dataset = prepare_dataset(eval_dataset_new, \"server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import gc\n",
    "from transformers import TrainingArguments, Trainer\n",
    "models_path = Path(\"./models\")\n",
    "logs_path = Path(\"./logs\")\n",
    "\n",
    "\n",
    "def train_classification(name: str, model, dataset, eval_dataset, epochs=2, batch_size=10):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    training_args = TrainingArguments(output_dir=str(models_path / name), overwrite_output_dir=True, num_train_epochs=epochs ,per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, \n",
    "    logging_dir=str(logs_path / name), logging_steps=2000, save_steps=30000, save_total_limit=5, evaluation_strategy=\"steps\", eval_steps=30000, learning_rate=2e-5, warmup_steps=1000, weight_decay=0.05, adam_beta1=0.9, adam_beta2=0.999, report_to=\"all\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda x: {\"accuracy\": (x.predictions.argmax(-1) == x.label_ids).mean()},\n",
    "    )\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: day_of_week, url, headline, authors, authors_gender, brief, date, content. If day_of_week, url, headline, authors, authors_gender, brief, date, content are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1783006\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 40\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 89152\n",
      "  Number of trainable parameters = 111181824\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/lnet/aic/personal/kydliceh/non_runable/NLP_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "model = get_model(6)\n",
    "train_classification(\"server\", model, server_dataset, server_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kydliceh/.cache/huggingface/datasets/dataset/default/0.0.0/3a21ebffd6f7380b097ffe2f8be2beda23497fd4b6ff06de19b9c473635bb199/cache-3584e9292ab6e185.arrow\n",
      "Loading cached processed dataset at /home/kydliceh/.cache/huggingface/datasets/dataset/default/0.0.0/3a21ebffd6f7380b097ffe2f8be2beda23497fd4b6ff06de19b9c473635bb199/cache-d237c5eda9ff5747.arrow\n"
     ]
    }
   ],
   "source": [
    "day_of_week_dataset = prepare_dataset(train_dataset_new, \"day_of_week\")\n",
    "day_of_week_eval_dataset = prepare_dataset(eval_dataset_new, \"day_of_week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_day \u001b[39m=\u001b[39m get_model(\u001b[39m7\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train_classification(\u001b[39m\"\u001b[39m\u001b[39mday_of_week\u001b[39m\u001b[39m\"\u001b[39m, model, server_dataset, server_eval_dataset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model_day = get_model(7)\n",
    "train_classification(\"day_of_week\", model_day, day_of_week_dataset, day_of_week_eval_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ef6dd07bac4272f716e725ea8b79349a1e1a5da9c3680ee7af3d80a9646d624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
