\documentclass{article}
\usepackage{biblatex}
\addbibresource{biblio.bib}
\usepackage{graphicx}
\graphicspath{{./Images}}
\usepackage{algpseudocode}
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{array}
\usepackage{svg}
\usepackage{hyperref}
\begin{document}
    \section{CZ check}
    We first run a check for cz language. For this purpose we used fasttext model.
    It can only work on lines so we had to split to lines and run it.
    At first we also applied the filtering to brief. But we found it that is had a lot a false negatives.
    We decided to drop it and oniy use the filterin on article
    This produces a percentage of czech lines. We tried to use ratio o 1.0 and checked the results.
    We got: #TODO
    It eliminiated a lot of articles so we decided to inspect it more closely. We generated histogram graphs based on ratio to see.
    We decided to inspect the result with ratio of 0.0. As expected we found a lot of ukraine article in seznamzpravy.cz
    Deník had a lot of articles with just one line with `VIDEOSOUHRN SPUSTíTE ZDE' etc...
    Idnes had problems with tables being parsed cell per line. So for mobile phones it had no chance to be czech.
    Other problems was english that was prevalent. We also found few slovak articles.

    0.3 outliers.
    Stil the same problems but more of false negatives. Often due to table or sport matches

    Matches problem:
    CELKOVÉ POŘADÍ - JEDNOTLIVCI
        1. a 2. místo
        TC0029 Milan Matoušek, Tachov 13 84
        TC0059 Radek Mazanec, Planá 13 84
        3. až 12. místo

    Tables problem



    We the proceed to NULL author that weren't real people.

    \section{Article inspection}
    After that we inspect articles per server basic


    \section{Irozhlas}
    We first inspect the oldest articles.
    It turns out there is one from 1992. Idk why it's there but i check url and it is really from that year. Should I erase all to 2017 because on github I said i will use only 2017-2019? Articles are correclty parsed because
    irozhlas has only one version feeelsgood.


    \subsection{headline}
    First we checked both ends of headlin we didn't find that the end of headline would be forceully cut off.
    Howerver there we problems at low number of characters. We found few articles with just two characters or one words.
    When preprocessing we would also split headline by [-] an then take a first part. However it turns out that it might not
    have been good idea as we would then get this headline. Fotbalisté Boleslavi deklasovali v play or Proč krachují start.
    We will have to run the extractor again I guess without this rule. After that we will set threshold to 25/30 characters.

    We also seen holes in the graph. However we listen the parts where there are supposed to be 0 headline length but there we no holes.
    Idk why it shows it like this..........

    We also checked why the mean line is so perfect around 2021 for mean by date.
    1 reason is simply because of graph density. Second is that the write are force to write healines of length 110 characters. We isnpected headlines and they were not cut off.
    We also checked the outliers with high values but they were perfeclty fine.

    \subsection{article length}
    No problem at big lengths. Those are podcast transcripts. I don't see reason to cut them off. As artciles over 4k words are jsut 5% of all articles.

    We found that articles with length < 100 were either bad parsed or just ome weird articles.
    This was also seen < 300. We decided to cut off articles with length < 300.

    We also inspected the big outlier with huge aritcle length that happend in 2022. It's this url and it is truly feaking long https://www.irozhlas.cz/zpravy-svet/polsko-evropska-unie-soudni-dvur-eu-europoslanec-marek-belka_2112151341_ern.


    \subsection{avg word length}
    We found out that low avg word length were the sport results. We decided to cut off articles with avg word length < 4.3. We haven't found anything wrong with high values. Otherwise we can see from graphs that there is high correlation between avg word length and article length.


    \subsection{word num}
    Since we could easly see huge correlation between word num and article length we decided to use ratio. We found the sport results articles to have ratio > 0.22.



    \subsection{brief}
    We found out that we could filter weather articles based on brief. noc -6 až -15 °C | den -9 až -5 °C. Also from what I have seen author article usually ahve longer brief than 50. If not it's usually sport report or aut oreport.
    No problems with long brief. We also check strange same brief at the 2022 but it is just bad visualization.


    \subsection{non-alpha}
    We found the non-alpha to be quite useless as it again corellated with article length. We decided to use ratio of non-alpha/article lengt. We found out that articles with 0.045 are the ones with sport results. low values not particularry interesting. We also observed the outlier in 2004 and those were again sport resuts.

    \subsection{num of words per line}
    We haven't found any interesting with high value. But we found that with value lower than 14 there we related articles list at the end. Lower were also weather prediction.


    \section{Idnes}
    We first inspect the oldest articles.
    First of all there is a article from 1900 which is href[https://www.idnes.cz/revue/zajimavosti/v-britanii-se-podarilo-vypestovat-dvojbarevne-jablko.A090928_105911_zajimavosti_nh this one. Probably a bug on idnes side. I removed it from analysis as it fucked up the graphs.]




    \subsection{headline}
    We first checked > 0.999 outliers. We didnt' find any probelms there. For < 20 it again showed the problem with [-]. We inspected 2014+ articles and didn't find any cutting

    We also checked drop after 74 but didn't find any cutting.
    \subsection{article length}
    No problem at big lengths. Those were some astornout lnog articles. I don't see reason to cut them off. As artciles over 6k words are jsut 5% of all articles.
    There was also one with sport results

    We found that articles with length < 100 were either bad parsed or just ome weird articles. <200 typicall short news.
    This was also seen < 300. We decided to cut off articles with length < 300.

    example of short one: https://www.idnes.cz/hry/kratke-zpravy/nadherny-pohled-na-theed-ve-star-wars-battlefront-ii.A170608_110306_bw-kratke-zpravy_oz

    example of short one at 199x https://www.idnes.cz/hry/recenze/quest-for-glory-v-dragon-fire.A000405_questforglory5_bw


    We also inspected strange outliers around 2000 but they were correctl parsed.



    \subsection{avg word length}
    We haven't found anything strange on both sides. We found some sport result <4 but it wasn't prevalent.

    \subsection{word num}
    Again we found 0.22 > ratio to be sport results Smaller.
    



    \subsection{brief}
        WE found that value < 20 were typically not articles but rather manuals or file pages eg čeština do hry x or it was weather

        We also check strange outliers around 2000 no idea if they are fine ???


    \subsection{non-alpha}
    Again 0.045 > ration was sport results. We have also checked peaks at < 2005 but they were correctly parsed sport articles.

    \subsection{num of words per line}
    Clealry there is problem with newlining at first extractor which ends at 2011. It doesn't correctly add \n. Will check it but I am not sure if it's even needed to fix it as we will just tokenize whole article and this should make a difference.

    WE also check the low outliers but those were probbaly wrongly parsed as some of them had like 10 words per line. Tho not problem if tokenized.

    \subsection{Key findings}
    Article length peaks are ok as per section. Same for brief length.
    Num words per line wrongly parsed line but should metter. non-alpha ratio is also fine

    \section{novinky}
    Again we found old article from 1900. I removed it from analysis as it fucked up the graphs.
    \subsection{headline}
        Nothing wrong with long ones
        Nothing interesint at == 20. But < 20 were strange.
        Nothing strange with them date > 2021.

    \subsection{article length}
        BIG PROBLEM !!! a lot of articles with < 200 length. Must cut it.
        Long articles not problematic as they are interviews.
        The problem is likely due to parsing as I found one article https://www.idnes.cz/onadnes/zdravi/pri-behani-hraje-nejvetsi-roli-hlava-osobni-zkusenosti.A160314_103053_ctenari-sobe_jid

        which only parsed the headings.
        Strange thing is that we have two extractors for novinky but the proportion of articles with < 200 length is the same. So it's not due to extractor.

    \subsection{avg word length}
        Outliers over 0.99 shows the problematinc articles with only heading.

    \subsection{word num ratio}
        Both high and low values shows problematic articles

    \subsection{brief}
        Overall brief looks kinda scatchy as they don't look like briefs. Low < 20 briefs show that. Will have to recheck extracting.

    \subsection{non-alpha}
        Again we can see clealry which articles are problematic. Because they have very low non-alpha ratio as those are only headings.
        No common denominator of high values.

    \subsection{num of words per line}
        Same problem low values show headings.
        
    \subsection{Key findings}
        BIG PROBLEM with content extraction oftentimes extracted only headings.
        No idea why there are holes in headline but shouldn't be there basedo on data. No way to inspect articles peak at 2018-07-19 as the articles are not there anymore.



    \section{SeznamZpravy}
    \subsection{headline}
        Classic - problem. No probelm otherwise. No idea why there is steap drop around 2019. INspected it and found nothing strange.
        No idea how to debug the spikes.


    \subsection{word length}
        Nothing interesting

    \subseciton{num words ratio}
        Same problem as with novinky > 0.22 shows only headings.

    \subsection{article length}
    The long articles at 2022 are due to covid articles or interviews.
    IIMPORTANT again with articles < 200 we spotteed problem with headings.
    We also check 

    \subsection{brief length}
    The step drop at 230 was inspected and is probably restricted by senzamzpravy as no cut was seen.


    \subsection{non-alpha}
    > 0.06 shows heading probllem.

    \subsection{num of words per line}
    No problem with parsing now. High values looked fine, just long paragraphs.
    Low values typically contained table.

    \subsection{Key findings}
    Again problem with heading. Brief is not probellmatic with drop.
    No idea how to debug steep drop at 2022-02 of num of articles.



    \section{aktualne}
    \subsection{headline}
     Same problem with [-]. Check the drops and haven't found any cutting.
     Check change by date 

     I have also checked the dramatic drops around 2008 and 2017.
     No idea what happend at 2008. I checked the headline before and after and they were fine.

     2017 was when layout of page change and we use extractor 2.0 for that. However I again checked both ends and they are correct articles thus it's likely that writers were told to write longer articles.

     Same applies to 2019.

    \subsection{article length}
    The long articles are transcripts of documents. like this https://zpravy.aktualne.cz/domaci/dokument-vladni-navrh-na-zruseni-delnicke-stany/r~i:article:649724/

    I think we should cut it by setting max length to something similiar to outher sources.

    < 200 are typically photogalleris or videos. We should cut it by setting min length to like 300. Maybe even higher

    \subsection{avg word length}
    No problem with high values. < 3.8 usually soprt results.

    \subsection{word num ratio}
    > 0.22 usually sport results. < 0.11 Usually short pages like photogallery or videos but not as prevalent as with aritcle length.

    \subsection{brief}
    Low values typically sport results live Slava - Spatta 1:2, but there not excluseive.

    We found high values like 300 to be valid briefs and correctly parsed. For some reason these long ones are from second extractor period. No idea why.

    FOUND IT. The difference is that there are basically two briefs for the article. One that is short is in metadata and one is at the start of the article. NEEDS TO BEEEE FIXED.

    \subsection{non-alpha}
    > 0.06 usually sport results

    \subsection{num of words per line}
    Nothing interesting

    \subsection{Key findings}
    Problem with brief. Must be extracted from article. Otherwise okay.
    Must cut short articles to elimate gallerys and videos.

    \sectoin{denik}
    \subsection{headline}
    Reason for peaks is simply that bin size is too big and thus some bins conainta only partial data. I checked 80 drop and haven't found any cutting.
    > 80 headlines were correctly pardsed. Same probles as other [-]

    \subsection{article length}
    No problems found at that department. Even < 200 were okay.

    \subsection{avg word length}
    > 10 problem as it was just urls. < 3.8  just sport results.

    \subsection{word num ratio}
    > 0.22 just sport results. < 0.11 just ulrs.

    \subsection{brief}
    Briefs correctly parsed.  Even ones with 1000 words. < 20 were fine too.

    \subsectoin{non-alpha}
    > 0.06 sport results/electoin results. < 0.01 not interesting.

    \subsection{num of words per line}
    < 10 table like articles with listings etc. Long one probably has wrong splitting, but no problem with tokenization.

    \subsection{Key findings}
    Overall it was very well parsed. The only problem are those super short articles with long word length.

    




    



    \printbibliography
\end{document}